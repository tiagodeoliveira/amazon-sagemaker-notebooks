{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c68c561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (4.62.3)\n",
      "Requirement already satisfied: timm in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.6.7)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (4.21.0)\n",
      "Requirement already satisfied: kaggle in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.5.12)\n",
      "Requirement already satisfied: albumentations in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.2.1)\n",
      "Requirement already satisfied: opencv-python-headless<4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (4.2.0.34)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from timm) (0.11.1)\n",
      "Requirement already satisfied: torch>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from timm) (1.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (6.1.2)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (1.26.8)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from albumentations) (0.18.3)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from albumentations) (1.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from qudida>=0.0.4->albumentations) (1.0.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (3.5.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (9.0.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (4.28.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tqdm timm transformers kaggle albumentations \"opencv-python-headless<4.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be9c09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading flickr-image-dataset.zip to /home/ec2-user/SageMaker/clip2\n",
      "100%|██████████████████████████████████████▉| 8.14G/8.16G [00:47<00:00, 203MB/s]\n",
      "100%|███████████████████████████████████████| 8.16G/8.16G [00:47<00:00, 185MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"tiagodeoliveira\"\n",
    "os.environ['KAGGLE_KEY'] = \"\"\n",
    "\n",
    "!kaggle datasets download -d hsankesara/flickr-image-dataset\n",
    "!unzip -q flickr-image-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17e8f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bca46328",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"flickr30k_images\"\n",
    "images_path = f\"{dataset_path}/{dataset_path}\" \n",
    "df = pd.read_csv(f\"{dataset_path}/results.csv\", delimiter=\"|\")\n",
    "df.columns = ['image', 'caption_number', 'caption']\n",
    "df['caption'] = df['caption'].str.lstrip()\n",
    "df['caption_number'] = df['caption_number'].str.lstrip()\n",
    "df.loc[19999, 'caption_number'] = \"4\"\n",
    "df.loc[19999, 'caption'] = \"A dog runs across the grass .\"\n",
    "ids = [id_ for id_ in range(len(df) // 5) for _ in range(5)]\n",
    "df['id'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cadaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPConfig:\n",
    "    temperature = 1.0\n",
    "    transform_size = 224\n",
    "    projection_dim = 256 \n",
    "    projection_dropout = 0.1\n",
    "    image_embedding_dim = 2048\n",
    "    text_embedding_dim = 768\n",
    "    text_encoder_model_name = 'gpt2' \n",
    "    image_encoder_model_name = 'resnet50'\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    head_lr = 1e-3\n",
    "    weight_decay = 1e-3\n",
    "    lr_scheduler_patience = 1\n",
    "    lr_scheduler_factor = 0.8\n",
    "    training_epochs = 4\n",
    "    data_loader_batch_size = 32\n",
    "    data_loader_workers = 4\n",
    "    data_loader_max_size = 50\n",
    "    token_max_length = 200\n",
    "    image_path = images_path\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2d3fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Utils\n",
    "######################\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(CLIPConfig.text_encoder_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "def get_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(CLIPConfig.transform_size, CLIPConfig.transform_size, always_apply=True),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def create_dataframes():\n",
    "    max_id = df[\"id\"].max() + 1\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = df[df[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = df[df[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "def cross_entropy(preds, targets):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    return (-targets * log_softmax(preds)).sum(1)\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "###################\n",
    "# Datasets\n",
    "##################\n",
    "\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=CLIPConfig.token_max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx]) for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{CLIPConfig.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=get_transforms(),\n",
    "    )\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CLIPConfig.data_loader_batch_size,\n",
    "        num_workers=CLIPConfig.data_loader_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader    \n",
    "\n",
    "########################################\n",
    "# Projection\n",
    "#######################################\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x    \n",
    "    \n",
    "####################\n",
    "# Encoders\n",
    "####################\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, model_name, trainable):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, True, num_classes=0, global_pool=\"avg\")\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name, trainable):\n",
    "        super().__init__()\n",
    "        self.model = GPT2Model.from_pretrained(model_name)\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]  \n",
    "\n",
    "###################################\n",
    "# Model\n",
    "##################################\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, temperature, image_embedding, text_embedding, trainable, image_encoder_model_name, text_encoder_model_name, projection_dim, projection_dropout):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(model_name=image_encoder_model_name, trainable=trainable)\n",
    "        self.text_encoder = TextEncoder(model_name=text_encoder_model_name, trainable=trainable)\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding, projection_dim=projection_dim, dropout=projection_dropout)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding, projection_dim=projection_dim, dropout=projection_dropout)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets)\n",
    "        images_loss = cross_entropy(logits.T, targets.T)\n",
    "        loss =  (images_loss + texts_loss) / 2.0\n",
    "        return loss.mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c638e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CLIPConfig.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CLIPConfig.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n",
    "\n",
    "def train_loop(train_loader, valid_loader):\n",
    "    print('Creating model....')\n",
    "    model = CLIPModel(\n",
    "        temperature=CLIPConfig.temperature, \n",
    "        image_embedding=CLIPConfig.image_embedding_dim, \n",
    "        text_embedding=CLIPConfig.text_embedding_dim, \n",
    "        trainable=True, \n",
    "        image_encoder_model_name=CLIPConfig.image_encoder_model_name, \n",
    "        text_encoder_model_name=CLIPConfig.text_encoder_model_name, \n",
    "        projection_dim=CLIPConfig.projection_dim, \n",
    "        projection_dropout=CLIPConfig.projection_dropout\n",
    "    ).to(CLIPConfig.device)\n",
    "    \n",
    "    print('Creating optimizer....')\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": CLIPConfig.image_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CLIPConfig.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(model.image_projection.parameters(), model.text_projection.parameters()), \"lr\": CLIPConfig.head_lr, \"weight_decay\": CLIPConfig.weight_decay}\n",
    "    ], weight_decay=0.)\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CLIPConfig.lr_scheduler_patience, factor=CLIPConfig.lr_scheduler_factor\n",
    "    )\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    print('Running....')\n",
    "    for epoch in range(CLIPConfig.training_epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = valid_epoch(model, valid_loader)\n",
    "        \n",
    "        if valid_loss.avg < best_loss:\n",
    "            best_loss = valid_loss.avg\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "            print(\"Saved Best Model!\")\n",
    "        \n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb74328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = create_dataframes()\n",
    "train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model....\n",
      "Creating optimizer....\n",
      "Running....\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c4afcada98425f9f468e7726072613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loop(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291f1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfdd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
