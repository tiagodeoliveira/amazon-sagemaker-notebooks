{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm timm transformers kaggle albumentations \"opencv-python-headless<4.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/docs/api\n",
    "!kaggle datasets download -d hsankesara/flickr-image-dataset\n",
    "![ ! -d  ./flickr30k_images ] && unzip -q flickr-image-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e8f6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5407/4262441599.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca46328",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"flickr30k_images\"\n",
    "images_path = f\"{dataset_path}/{dataset_path}\" \n",
    "df = pd.read_csv(f\"{dataset_path}/results.csv\", delimiter=\"|\")\n",
    "df.columns = ['image', 'caption_number', 'caption']\n",
    "df['caption'] = df['caption'].str.lstrip()\n",
    "df['caption_number'] = df['caption_number'].str.lstrip()\n",
    "df.loc[19999, 'caption_number'] = \"4\"\n",
    "df.loc[19999, 'caption'] = \"A dog runs across the grass .\"\n",
    "ids = [id_ for id_ in range(len(df) // 5) for _ in range(5)]\n",
    "df['id'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cadaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPConfig:\n",
    "    temperature = 1.0\n",
    "    transform_size = 224\n",
    "    projection_dim = 256 \n",
    "    projection_dropout = 0.1\n",
    "    image_embedding_dim = 2048\n",
    "    text_embedding_dim = 768\n",
    "    text_encoder_model_name = 'gpt2' \n",
    "    image_encoder_model_name = 'resnet50'\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    head_lr = 1e-3\n",
    "    weight_decay = 1e-3\n",
    "    lr_scheduler_patience = 1\n",
    "    lr_scheduler_factor = 0.8\n",
    "    training_epochs = 4\n",
    "    data_loader_batch_size = 40\n",
    "    data_loader_workers = 5\n",
    "    token_max_length = 200\n",
    "    image_path = images_path\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d3fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Utils\n",
    "######################\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(CLIPConfig.text_encoder_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "def get_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(CLIPConfig.transform_size, CLIPConfig.transform_size, always_apply=True),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def create_dataframes():\n",
    "    max_id = df[\"id\"].max() + 1\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = df[df[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = df[df[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "def cross_entropy(preds, targets):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    return (-targets * log_softmax(preds)).sum(1)\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "###################\n",
    "# Datasets\n",
    "##################\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=CLIPConfig.token_max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx]) for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{CLIPConfig.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=get_transforms(),\n",
    "    )\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CLIPConfig.data_loader_batch_size,\n",
    "        num_workers=CLIPConfig.data_loader_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader    \n",
    "\n",
    "########################################\n",
    "# Projection\n",
    "#######################################\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x    \n",
    "    \n",
    "####################\n",
    "# Encoders\n",
    "####################\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, model_name, trainable):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, True, num_classes=0, global_pool=\"avg\")\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name, trainable):\n",
    "        super().__init__()\n",
    "        self.model = GPT2Model.from_pretrained(model_name)\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]  \n",
    "\n",
    "###################################\n",
    "# Model\n",
    "##################################\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, temperature, image_embedding, text_embedding, trainable, image_encoder_model_name, text_encoder_model_name, projection_dim, projection_dropout):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(model_name=image_encoder_model_name, trainable=trainable)\n",
    "        self.text_encoder = TextEncoder(model_name=text_encoder_model_name, trainable=trainable)\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding, projection_dim=projection_dim, dropout=projection_dropout)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding, projection_dim=projection_dim, dropout=projection_dropout)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets)\n",
    "        images_loss = cross_entropy(logits.T, targets.T)\n",
    "        loss =  (images_loss + texts_loss) / 2.0\n",
    "        return loss.mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c638e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CLIPConfig.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CLIPConfig.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n",
    "\n",
    "def train_loop(train_loader, valid_loader, model, optimizer, scheduler):\n",
    "    best_loss = float('inf')\n",
    "    print('Running....')\n",
    "    for epoch in range(CLIPConfig.training_epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = valid_epoch(model, valid_loader)\n",
    "        \n",
    "        if valid_loss.avg < best_loss:\n",
    "            print('Saving best model...')\n",
    "            best_loss = valid_loss.avg\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "        \n",
    "        scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81381394-0ba6-49bc-88bf-2330beb0a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = create_dataframes()\n",
    "train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b399835-d660-411d-b0e8-1cf2acab7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel(\n",
    "    temperature=CLIPConfig.temperature, \n",
    "    image_embedding=CLIPConfig.image_embedding_dim, \n",
    "    text_embedding=CLIPConfig.text_embedding_dim, \n",
    "    trainable=True, \n",
    "    image_encoder_model_name=CLIPConfig.image_encoder_model_name, \n",
    "    text_encoder_model_name=CLIPConfig.text_encoder_model_name, \n",
    "    projection_dim=CLIPConfig.projection_dim, \n",
    "    projection_dropout=CLIPConfig.projection_dropout\n",
    ").to(CLIPConfig.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c09fd793-1ad1-4f3d-b233-b8acf744bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.image_encoder.parameters(), \"lr\": CLIPConfig.image_encoder_lr},\n",
    "    {\"params\": model.text_encoder.parameters(), \"lr\": CLIPConfig.text_encoder_lr},\n",
    "    {\"params\": itertools.chain(model.image_projection.parameters(), model.text_projection.parameters()), \"lr\": CLIPConfig.head_lr, \"weight_decay\": CLIPConfig.weight_decay}\n",
    "], weight_decay=0.)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=CLIPConfig.lr_scheduler_patience, factor=CLIPConfig.lr_scheduler_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86661068c2747b29a3b28bcdce0035b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e86ac9bef934b039412d5e361d74c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/795 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr_scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5407/1124966561.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5407/2623607189.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, valid_loader, model, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "train_loop(train_loader, valid_loader, model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1da02-5d21-4d0c-b33e-10510d53e786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
